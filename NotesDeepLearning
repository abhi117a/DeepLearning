Loss Function:
Regression
MSE - Mean Squared Error
Gradient Descent

Classification
Softmax -
        We'll discuss one of the most important-for-us methods, logistic regression. And to talk about it, we should first find a way to convert our scores from linear classifiers, to probabilities, to distribution. So, we have some vector of scores z, which has components w transposed by x, though these are scores for each of our classes. Dot products can have any sign and have any magnitude. So we cannot interpret them as probability distributions, and we should somehow change it. We'll do it in two steps. At first step, we take first component of our vector and take e to the degree of this component. We do the same to the second component, et cetera, to the last component. So, after this step, we have a vector e to the degree of z that has only positive coordinates. So now we need only to normalize these components to get a distribution. And to do it, we just sum all the components of this e-to-z vector and divide each component by the sum. And after that, we get a vector sigma of z that is normalized and has only non-negative components. So we can interpret it as a probability distribution. This transform is called a softmax function -- a softmax transform.

        1) take exponential of the every value.
        2) Sum all the values of the exponent
        3) divide the exponent by the sum from step 2

Sigmoid
Cross Entropy
Gradient Descent


Regularization:
L1 and L2 penalty

it peanlizes the model if we use complex models



////
Supervised Neural Nets
ANN - Used for Classification and Regression
CNN - Computer Vision
RNN - Time series analysis

Unsupervised Neural Nets
Self Organizing Maps - Used for feature detections
Deep Boltzman machines - Recommendation Algo
Auto Encoders - Recommendation Algo


RNNs
Vanishing Gradient Problem:
If the starting weights are low (Wrec), during back propogation they start
converging to 0 which forces the network layers which are near the begining
being untrained due to reducing gradient, which in return gives untrained values during forward
propagation of the network. Starts a domino effect.
  Solution:
          Weight Initialization
          Echo State Network
          Long Short Term Memory Networks (LSTM)

But if Wrec is very large we start getting Exploding gradient problem
    Solution: Truncated Backpropagation
              Penalties
              Gradient clipping (setting threshold of gradient)


LSTM - Long Short Term Memory
